import akshare as ak
import pandas as pd
import os
import json
import time
import random
from datetime import datetime
from utils import logger, get_beijing_time

class NewsLoader:
    def __init__(self):
        # æ•°æ®å­˜å‚¨ç›®å½•
        self.CACHE_DIR = "data_news"
        if not os.path.exists(self.CACHE_DIR):
            os.makedirs(self.CACHE_DIR)
        
        # æŒ‰æ—¥æœŸåˆ†æ–‡ä»¶å­˜å‚¨ï¼Œä¾‹å¦‚: data_news/news_2026-02-09.jsonl
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´ç¡®ä¿æ—¥æœŸå‡†ç¡®
        self.today_str = get_beijing_time().strftime("%Y-%m-%d")
        self.file_path = os.path.join(self.CACHE_DIR, f"news_{self.today_str}.jsonl")

    def _load_existing_titles(self):
        """
        è¯»å–å·²å­˜åœ¨çš„æ–°é—»æ ‡é¢˜ï¼Œç”¨äºå¢é‡å»é‡
        """
        titles = set()
        if os.path.exists(self.file_path):
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line: continue
                        try:
                            data = json.loads(line)
                            titles.add(data.get('title', '').strip())
                        except: 
                            continue
            except Exception as e:
                logger.warning(f"è¯»å–å†å²æ–°é—»æ–‡ä»¶å‡ºé”™: {e}")
        return titles

    def fetch_and_save(self):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šæŠ“å– -> å»é‡ -> è¿½åŠ å†™å…¥
        """
        existing_titles = self._load_existing_titles()
        new_items = []
        
        logger.info(f"ğŸ“¡ [NewsLoader] å¼€å§‹å¢é‡æŠ“å–æ–°é—» ({self.today_str})...")
        
        # --- æ•°æ®æº: ä¸œè´¢è´¢ç»å¯¼è¯» ---
        try:
            # éšæœºå»¶æ—¶é˜²åçˆ¬
            time.sleep(random.uniform(2.0, 5.0))
            
            # è·å–æœ€æ–°çš„è´¢ç»è¦é—»
            df = ak.stock_news_em(symbol="è¦é—»")
            
            # å…¼å®¹åˆ—å
            title_col = 'æ–°é—»æ ‡é¢˜' if 'æ–°é—»æ ‡é¢˜' in df.columns else 'title'
            time_col = 'å‘å¸ƒæ—¶é—´' if 'å‘å¸ƒæ—¶é—´' in df.columns else 'public_time'
            content_col = 'æ–°é—»å†…å®¹' if 'æ–°é—»å†…å®¹' in df.columns else 'content'
            
            count = 0
            for _, row in df.iterrows():
                title = str(row.get(title_col, '')).strip()
                pub_time = str(row.get(time_col, ''))
                content = str(row.get(content_col, '')).strip()
                
                # ç®€å•æ¸…æ´—ï¼šå»é™¤æ— æ•ˆæ ‡é¢˜
                if not title or title == 'nan': continue
                if len(title) < 5: continue
                
                # å»é‡æ£€æŸ¥
                if title not in existing_titles:
                    new_items.append({
                        "source": "EastMoney",
                        "time": pub_time,
                        "title": title,
                        "content": content[:200] # åªå­˜æ‘˜è¦ï¼ŒèŠ‚çœç©ºé—´ï¼Œä¸»è¦é æ ‡é¢˜
                    })
                    existing_titles.add(title)
                    count += 1
            
            logger.info(f"âœ… ä»ä¸œè´¢è·å–åˆ° {count} æ¡æ–°æ¶ˆæ¯")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸œè´¢æ–°é—»æŠ“å–å—é˜»: {e}")

        # --- (å¯é€‰) åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–æ•°æ®æº ---
        
        # --- ä¿å­˜å…¥åº“ ---
        if new_items:
            # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œä¿è¯æ–‡ä»¶å†…æœ‰åº
            new_items.sort(key=lambda x: x['time'])
            
            try:
                with open(self.file_path, 'a', encoding='utf-8') as f:
                    for item in new_items:
                        f.write(json.dumps(item, ensure_ascii=False) + "\n")
                
                logger.info(f"ğŸ’¾ [NewsLoader] æˆåŠŸå…¥åº“ {len(new_items)} æ¡æ–°é—» -> {self.file_path}")
            except Exception as e:
                logger.error(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        else:
            logger.info("ğŸ’¤ [NewsLoader] æš‚æ— æ–°æ¶ˆæ¯ï¼Œæ–‡ä»¶æœªæ›´æ–°ã€‚")

if __name__ == "__main__":
    loader = NewsLoader()
    loader.fetch_and_save()
