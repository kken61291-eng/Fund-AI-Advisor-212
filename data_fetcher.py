import akshare as ak
import pandas as pd
import time
import random
import os
from datetime import datetime, time as dt_time
from utils import logger, retry, get_beijing_time

class DataFetcher:
    def __init__(self):
        # [V15.13] æœ¬åœ°æ•°æ®ä»“åº“é…ç½®
        self.DATA_DIR = "data_cache"
        if not os.path.exists(self.DATA_DIR):
            os.makedirs(self.DATA_DIR)
            
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]

    def _verify_data_freshness(self, df, fund_code, source_name):
        """æ•°æ®æ–°é²œåº¦å®¡è®¡ (é€šç”¨)"""
        if df is None or df.empty: return
        
        last_date = pd.to_datetime(df.index[-1]).date()
        now_bj = get_beijing_time()
        today_date = now_bj.date()
        is_trading_time = (dt_time(9, 30) <= now_bj.time() <= dt_time(15, 0))
        
        log_prefix = f"ğŸ“… [{source_name}] {fund_code} æœ€æ–°æ—¥æœŸ: {last_date}"
        
        if last_date == today_date:
            logger.info(f"{log_prefix} | âœ… æ•°æ®å·²æ›´æ–°è‡³ä»Šæ—¥")
        elif last_date < today_date:
            days_gap = (today_date - last_date).days
            if is_trading_time and days_gap >= 1:
                logger.warning(f"{log_prefix} | âš ï¸ æ•°æ®æ»å {days_gap} å¤© (è¯·è¿è¡Œ batch_updater æ›´æ–°æ•°æ®)")
            else:
                logger.info(f"{log_prefix} | â¸ï¸ å†å²æ•°æ®å°±ç»ª")

    @retry(retries=3, delay=5)
    def _fetch_from_network(self, fund_code):
        """
        [ç§æœ‰æ–¹æ³•] çº¯è”ç½‘è·å–æ•°æ® (ä¸œè´¢ -> æ–°æµª -> è…¾è®¯)
        ä¾› batch_updater è°ƒç”¨ï¼Œmain.py ä¸ç›´æ¥è°ƒç”¨æ­¤æ–¹æ³•
        """
        # 1. ä¸œè´¢
        try:
            # å³ä½¿åœ¨çˆ¬è™«è„šæœ¬é‡Œï¼Œä¹Ÿä¿ç•™éšæœºå»¶æ—¶ï¼Œæ¨¡æ‹ŸçœŸäºº
            time.sleep(random.uniform(1.0, 2.0)) 
            df = ak.fund_etf_hist_em(symbol=fund_code, period="daily", start_date="20240101", end_date="20500101", adjust="qfq")
            rename_map = {'æ—¥æœŸ':'date', 'å¼€ç›˜':'open', 'æ”¶ç›˜':'close', 'æœ€é«˜':'high', 'æœ€ä½':'low', 'æˆäº¤é‡':'volume'}
            df.rename(columns=rename_map, inplace=True)
            df['date'] = pd.to_datetime(df['date'])
            df.set_index('date', inplace=True)
            if not df.empty: return df, "ä¸œè´¢"
        except: pass

        # 2. æ–°æµª
        try:
            time.sleep(1)
            df = ak.fund_etf_hist_sina(symbol=fund_code)
            if df.index.name in ['date', 'æ—¥æœŸ']: df = df.reset_index()
            if len(df.columns) >= 6:
                df.columns = ['date', 'open', 'high', 'low', 'close', 'volume'] + list(df.columns[6:])
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                # ç±»å‹æ¸…æ´—
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')
                return df, "æ–°æµª"
        except: pass

        # 3. è…¾è®¯
        try:
            time.sleep(1)
            prefix = 'sh' if fund_code.startswith('5') else ('sz' if fund_code.startswith('1') else '')
            if prefix:
                df = ak.stock_zh_a_hist_tx(symbol=f"{prefix}{fund_code}", start_date="20240101", adjust="qfq")
                rename_map = {'æ—¥æœŸ':'date', 'å¼€ç›˜':'open', 'æ”¶ç›˜':'close', 'æœ€é«˜':'high', 'æœ€ä½':'low', 'æˆäº¤é‡':'volume'}
                df.rename(columns=rename_map, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
                if not df.empty: return df, "è…¾è®¯"
        except: pass
        
        return None, None

    def update_cache(self, fund_code):
        """
        [çˆ¬è™«ä¸“ç”¨] è”ç½‘ä¸‹è½½æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ° CSV
        """
        df, source = self._fetch_from_network(fund_code)
        if df is not None and not df.empty:
            file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
            df.to_csv(file_path)
            logger.info(f"ğŸ’¾ [{source}] {fund_code} æ•°æ®å·²ä¿å­˜è‡³ {file_path}")
            return True
        else:
            logger.error(f"âŒ {fund_code} æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥ï¼Œæ— æ³•æ›´æ–°ç¼“å­˜")
            return False

    def get_fund_history(self, fund_code, days=250):
        """
        [ä¸»ç¨‹åºä¸“ç”¨] åªè¯»æ¨¡å¼ï¼šç›´æ¥ä»æœ¬åœ° CSV è¯»å–æ•°æ®
        """
        file_path = os.path.join(self.DATA_DIR, f"{fund_code}.csv")
        
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ æœ¬åœ°ç¼“å­˜ç¼ºå¤±: {fund_code}ï¼Œè¯·å…ˆè¿è¡Œ batch_updater.py")
            return None
            
        try:
            # è¯»å– CSV
            df = pd.read_csv(file_path)
            
            # è¿˜åŸç´¢å¼•å’Œæ•°æ®ç±»å‹
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
            
            self._verify_data_freshness(df, fund_code, "æœ¬åœ°ç¼“å­˜")
            return df
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æœ¬åœ°ç¼“å­˜å¤±è´¥ {fund_code}: {e}")
            return None
